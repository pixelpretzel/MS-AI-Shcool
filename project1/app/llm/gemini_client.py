# app/llm/gemini_client.py
import os
import google.generativeai as genai

from app.config import GEMINI_API_KEY


def build_sd_prompt_from_text(ocr_text):
    
    system_prompt = """
Role: You are an expert prompt engineer for Stable Diffusion who turns childrenâ€™s story text into consistent, detailed illustration prompts.
Task: Based on the input text, write one clear, concise English prompt for image generation that faithfully depicts a single key scene (including main characters, their consistent appearance, emotions, actions, setting, time of day, atmosphere), without any meta-commentary or instructions.

CRITICAL RULES (Must Follow): 
1. DYNAMIC ENTITY DETECTION: 
- Determine if the subject is Human or Animal from context. 
- BIAS FIX: If the subject is a human role (e.g., King, Queen, Student), MUST add "Human" prefix (e.g., "Human King"). 
- ANIMAL FIX: If it is an animal, specify the species clearly (e.g., "Baby Bear animal"). 
2. QUANTITY FIX: If a number is mentioned for the Main Subject, use digits in parentheses at the start (e.g., "(5) baby ducks"). 
3. SAFETY & EMOTION: Convert scary/violent actions into child-friendly facial expressions or poses (e.g., "crying" -> "sad face", "fighting" -> "standing confidently"). 

Output Format: [Quantity if any] [Adjective] [ONE Main Subject] [Action] [Simple Background] (Do not add any other words, explanations, or intros.)

"""

    user_prompt = f"Input text: {ocr_text}"

    model = genai.GenerativeModel("gemini-2.5-flash")
    response = model.generate_content(
        [
            system_prompt,
            user_prompt,
        ]
    )

    return (response.text or "").strip()

def build_ai_question(ocr_text):
    question_prompt = """
**Role:** You are a highly specialized AI designed to serve as the core function of an **'AI-Powered Reading Companion'** for young children.

**Objective:** Your primary task is to analyze a given children's story text (fairytale/picture book content) and generate data that specifically aids the language comprehension and developmental needs of children aged **3 to 7 years old**, who require visual information for better understanding.

**Core Output Directives:**
You must produce a structured output focused on promoting active engagement and linguistic development.

---

### **Language Development Questions (5 Total)**

* Generate **exactly five (5) high-quality questions** to facilitate an engaging dialogue with the child.
* The questions must cover the following **five mandatory and distinct developmental areas** to ensure diversity and creativity:
    1.  **Text Comprehension & Recall:** A question focused on **recalling the main content, characters, or setting** (Who, What, Where, When).
    2.  **Inference & Emotional Literacy:** A question about **inferring a character's feelings, motivations, or intentions**, requiring the child to understand 'why' a character acted a certain way or 'how' they felt.
    3.  **Creative Prediction & Alternative Ending:** A question that encourages the child to **imagine what happens next** in the story or **propose a new, creative outcome or alternative ending.**
    4.  **Vocabulary & Sensory Detail:** A question that prompts the child to **use a specific, newly introduced vocabulary word** from the text, or describe the story using **sensory details** (e.g., "What colors did you see?" "What sound did X make?").
    5.  **Personal Connection & Role-Playing ('What if I were'):** A personalized question (e.g., **"If you were the character, what would you do differently?"** or **"What part of the story reminds you of your own experience?"**).
    * *Example Output Format (MUST BE IN INFORMAL KOREAN. ë°˜ë§ë¡œ ì‘ì„±í•´ì¤˜.):*
        * Q1. [ì§ˆë¬¸ í…ìŠ¤íŠ¸(ë°˜ë§)]
        * Q2. [ì§ˆë¬¸ í…ìŠ¤íŠ¸(ë°˜ë§)]
        * Q3. [ì§ˆë¬¸ í…ìŠ¤íŠ¸(ë°˜ë§)]
        * Q4. [ì§ˆë¬¸ í…ìŠ¤íŠ¸(ë°˜ë§)]
        * Q5. [ì§ˆë¬¸ í…ìŠ¤íŠ¸(ë°˜ë§)]

---
"""

    user_prompt = f"Input text: {ocr_text}"

    model = genai.GenerativeModel("gemini-2.5-flash")

    response = model.generate_content(                                              [                                                                               question_prompt,                                                            user_prompt,                                                            ]                                                                       )

    return response.text.strip()


# --------------------------
# ğŸ’¬ ì•„ì´ ë‹µì¥ì— ë¦¬ì•¡ì…˜í•˜ëŠ” ì±„íŒ…ìš© í•¨ìˆ˜
# --------------------------

STORY_TEACHER_SYSTEM_PROMPT = """
ë„ˆëŠ” ê·¸ë¦¼ì±…ì„ í•¨ê»˜ ì½ì–´ì£¼ëŠ” ë‹¤ì •í•œ ì„ ìƒë‹˜ì´ì•¼.
3~7ì‚´ ì•„ì´ì™€ ì´ì•¼ê¸° ë‚˜ëˆ„ë“¯ì´ ëŒ€í™”í•´.
í•­ìƒ í¸ì•ˆí•œ ë°˜ë§ì„ ì“°ê³ , ì§§ê²Œ 1~3ë¬¸ì¥ ì •ë„ë¡œ ëŒ€ë‹µí•´.
ì•„ì´ì˜ ëŒ€ë‹µì„ ì˜ ë°›ì•„ì£¼ê³ , ê°€ë”ì€ ë‹¤ì‹œ ë¬¼ì–´ë³´ë©´ì„œ ëŒ€í™”ë¥¼ ì´ì–´ê°€.
AIë‚˜ ëª¨ë¸ì´ë¼ëŠ” ë§ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆ.
"""


def build_chat_reaction(child_message: str, history: list[dict]) -> str:
    """
    ì•„ì´ê°€ ë³´ë‚¸ ìµœì‹  ë©”ì‹œì§€ + ì´ì „ historyë¥¼ ë°”íƒ•ìœ¼ë¡œ
    'ì„ ìƒë‹˜' ì—­í• ì˜ ë°˜ë§ ë¦¬ì•¡ì…˜ì„ ìƒì„±.
    history í˜•ì‹ ì˜ˆ:
      [
        {"role": "assistant", "content": "ëŠ‘ëŒ€ê°€ ë‚˜íƒ€ë‚˜ì„œ ì•„ê¸° ë¼ì§€ëŠ” ê¸°ë¶„ì´ ì–´ë• ì„ê¹Œ?"},
        {"role": "user", "content": "ë¬´ì„œì› ì„ ê²ƒ ê°™ì•„."}
      ]
    """

    model = genai.GenerativeModel("gemini-2.5-flash")

    # ëŒ€í™” ë¡œê·¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì´ì–´ë¶™ì´ê¸°
    conv_lines = []

    for turn in history:
        role = turn.get("role")
        content = turn.get("content", "")

        if role == "user":
            conv_lines.append(f"ì•„ì´: {content}")
        elif role == "assistant":
            conv_lines.append(f"ì„ ìƒë‹˜: {content}")
        else:
            conv_lines.append(content)

    # ìµœì‹  ì•„ì´ ë©”ì‹œì§€ëŠ” history ë°”ê¹¥ì—ì„œ ë°›ì€ ê²ƒìœ¼ë¡œ ì²˜ë¦¬
    conv_lines.append(f"ì•„ì´: {child_message}")

    conversation_text = "\n".join(conv_lines)

    prompt = f"""
{STORY_TEACHER_SYSTEM_PROMPT}

ì•„ë˜ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ì•¼. ë§ˆì§€ë§‰ ì¤„ì˜ 'ì•„ì´' ë§ì— ì´ì–´ì„œ,
'ì„ ìƒë‹˜' ì…ì¥ì—ì„œ ë”°ëœ»í•˜ê²Œ ë°˜ë§ë¡œ 1~3ë¬¸ì¥ ì •ë„ë¡œ ëŒ€ë‹µí•´ì¤˜.

ëŒ€í™”:
{conversation_text}

ì£¼ì˜ì‚¬í•­:
- ì•„ì´ì˜ ë§ì„ ë¨¼ì € ê³µê°í•´ ì£¼ê³ , í•„ìš”í•˜ë©´ ì‰¬ìš´ ì§ˆë¬¸ì„ í•œ ë²ˆ ë” í•´ ì¤˜.
- ì´ëª¨ì§€ ì“°ì§€ ë§ˆ.
"""

    response = model.generate_content(prompt)
    return (response.text or "").strip()

def summarize_chat_history(history: list[dict]) -> str:
    """
    ì•„ì´ì™€ ì„ ìƒë‹˜ ì‚¬ì´ì˜ ëŒ€í™” historyë¥¼ ë°›ì•„
    ì•„ì´ê°€ ì–´ë–¤ ìƒê°/ê°ì •ì„ ë§í–ˆëŠ”ì§€ ì¤‘ì‹¬ìœ¼ë¡œ ì§§ê²Œ ìš”ì•½í•´ì¤€ë‹¤.

    history ì˜ˆ:
    [
      {"role": "assistant", "content": "ëŠ‘ëŒ€ê°€ ë‚˜íƒ€ë‚¬ì„ ë•Œ ì•„ê¸° ë¼ì§€ëŠ” ì–´ë–¤ ê¸°ë¶„ì´ì—ˆì„ê¹Œ?"},
      {"role": "user", "content": "ë¬´ì„œì› ì„ ê²ƒ ê°™ì•„."},
      {"role": "assistant", "content": "ê·¸ë ‡êµ¬ë‚˜, ë¬´ì„œì› ê² êµ¬ë‚˜. ë„ˆë¼ë©´ ì–´ë–»ê²Œ í–ˆì„ ê²ƒ ê°™ì•„?"},
      {"role": "user", "content": "ë‚˜ëŠ” ë„ë§ê°”ì„ ê²ƒ ê°™ì•„."}
    ]
    """

    model = genai.GenerativeModel("gemini-2.5-flash")

    conv_lines = []
    for turn in history:
        role = turn.get("role")
        content = turn.get("content", "")
        if role == "user":
            conv_lines.append(f"ì•„ì´: {content}")
        elif role == "assistant":
            conv_lines.append(f"ì„ ìƒë‹˜: {content}")
        else:
            conv_lines.append(content)

    convo_text = "\n".join(conv_lines)

    prompt = f"""
ë„ˆëŠ” 3~7ì‚´ ì•„ì´ì™€ ê·¸ë¦¼ì±…ì„ ì½ê³  ëŒ€í™”í•œ ë‚´ìš©ì„ ì •ë¦¬í•´ ì£¼ëŠ” ì„ ìƒë‹˜ì´ì•¼.

ì•„ë˜ëŠ” ì„ ìƒë‹˜(assistant)ê³¼ ì•„ì´(user)ì˜ ëŒ€í™” ê¸°ë¡ì´ì•¼.
ì´ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ,
1) ì•„ì´ê°€ ì–´ë–¤ ê°ì •/ìƒê°/ê´€ì ì„ ë§í–ˆëŠ”ì§€
2) ì–´ë–¤ ì£¼ì œ(ìƒí™©)ì— ëŒ€í•´ ì´ì•¼ê¸°í–ˆëŠ”ì§€
ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì§§ê²Œ ìš”ì•½í•´ì¤˜.

í˜•ì‹:
- 2~4ë¬¸ì¥ ì •ë„ì˜ ê°„ë‹¨í•œ í•œêµ­ì–´ ë¬¸ë‹¨
- ì•„ì´ê°€ í•œ ë§ì€ "ì•„ì´ì˜ ë§ì— ë”°ë¥´ë©´ ~" ì´ëŸ° ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜.
- ì „ì²´ì ì¸ ë¶„ìœ„ê¸°(ì¬ë°Œì–´í•¨, ë¬´ì„œì›Œí•¨, ê³µê°í•¨ ë“±)ë„ í•œ ì¤„ ì–¸ê¸‰í•´ì¤˜.
- ë°˜ë§ ë§ê³ , êµì‚¬ìš© ê¸°ë¡ì²˜ëŸ¼ ì •ì¤‘í•œ ì„œìˆ ì²´ë¡œ ì‘ì„±í•´.

ëŒ€í™” ê¸°ë¡:
{convo_text}
"""

    response = model.generate_content(prompt)
    return (response.text or "").strip()
